动机：解决什么问题
解决方案：创新点，什么架构，什么方法

# Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation（cvpr2023）
## 解决问题
- 提取运动信息和外观信息的两种方法：
	- 混合提取：直接将两帧按通道拼接在一起，经过重复的网络模块提取特征。 
		- 缺点1：对特征提取模块的设计和容量有较高的要求
		- 缺点2：因为没有显示的运动信息，无法直接得到任意时刻运动建模所欲的运动信息，这限制了任意时刻插帧的能力
	- 串行提取：首先提取每一帧单独外观信息，再利用两者外观信息提取运动信息。
		- 缺点1：需要针对每一种信息单独设计提取模块，引入额外计算开销
		- 缺点2：无法像混合提取一样只需堆叠相同模块就可以提高性能
		- 缺点3：得到的外观特征没有很好的进行帧间信息交互，而这种信息交互对于生成中间帧至关重要
- **本文提出了一个模块能够同时显式地提取两种信息，并且可以像混合提取那样通过控制模块的个数和容量来控制性能**
	- 优点1：每一帧的外观特征可以相互增强，但不与运动特征混合，以保留详细的静态结构信息
	- 优点2： 所获得的运动特征可以按时间进行缩放，然后作为线索，指导在输入帧之间的任意时刻的帧的生成
	- 优点3：只需要控制模块的复杂性和模块的数量，以平衡整体性能和推理速度
## 解决方案
- 使用CNN提取高分辨率图像的low-level信息。然后使用帧间注意力机制的Transformer块提取低分辨率下运动特征和帧间外观信息。
![帧间注意力](images/ema-vfi1.png)

- 当前帧中的一个区域作为query，而另外相邻帧的所有区域作为key和value。推导出当前区域和另一帧相邻区域的注意力图。该注意力图被用来汇总邻居的外观特征并当前区域的外观特征聚合得到**同一个区域在两帧不同位置的外观信息的聚合特征**，同时该注意力图也被用来对另一帧的相邻区域的位移进行加权得到**当前区域从当前帧到相邻帧的近似运动向量**

![](images/EMA-VFI2.png)

## 缺点
- 尽管混合CNN和Transformer的设计可以减轻计算开销，但它也限制了在的高分辨率外观特征下利用IFA进行运动信息的提取
- 该方法仅限于两个连续帧，无法利用多个连续帧的信息



# AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation（cvpr2023）
## 解决问题
- 现存的插帧预测方法所预测的光流与实际运动不够一致，尤其是运动比较大的时候。而且传统方法难以处理遮挡和运动边界细节。
- 本文提出了一种新的网络架构解决了运动幅度过大和有遮挡的问题

## 解决方案
![](amt1.png)

- correlation encoder : 将输入帧映射到到一对密集特征用来构建双向相关体
- context encoder ： 输出初始插值的中间特征并预测初始化的双边光流$F^1_{t→0}$ 和 $F^1_{t→1}$ 。 同时提取特征金字塔用于更深层次的warping
- 使用预测的双边光流检索相关性来更新流和插值的中间特征
- 从粗双边流中推导多组细粒度流场以便单独对输入帧进行反向扭曲
- 混合使用了CNN和Transform架构



# A Unified Pyramid Recurrent Network for Video Frame Interpolation（cvpr2023）

## 解决问题
- 在传统的光流合成任务中，光流会由一个金字塔网络从粗到细进行估计，但是中间帧只会被合成网络合成一次。这种方式在在低分辨率的视频上有效，但是在***高分辨率***的视频上，这种方法错过了迭代细化过程中插值信息
- 当运动幅度比较大时，插入的中间帧会产生明显的***伪影***
- 现有方法模型架构复杂，很难部署到有限资源的设备上
- 本文针对上述问题提出了一种新颖的**统一金字塔循环网络** 进行双向光流估计和基于forward-warping的帧合成。在光流迭代细化的同时迭代细化中间帧。

## 解决方案
![](upr01.png)
- 首先对给定的两帧输入构建图金字塔。然后对金字塔不同层进行循环估计双向光流和中间帧。该循环结构具体包括一个特征encoder层进行多尺度特征提取，一个双向光流模型用于估计双向光流，一个帧合成模型利用forward-warping合成中间帧。这两个模型共享金字塔层提供的权重。
- 其中，在每个金字塔层，使用CNN对一对输入进行特征提取作为encoder。将最后一个encoder层提取的特征和光流从前一层上采样得到的特征通过双向光流模型处理得到精细化的光流。得到的光流又可以用于输入帧的forward-warp和多尺度CNN特征的更新。最后，帧合成模型结合forward-warp的结果和上一次的中间帧上采样的结果产生中间帧。
![](upr02.png)
- 双向光流模型
![](upr03.png)
- 基于U-Net的帧合成模块
- encoder部分包含三个卷积模块，用于下采样。decoder层也包含三个转置卷积模块用于上采样


作者认为：可以假设在较低分辨率的情况下运动幅度比较小，这样不会产生伪影。如此的话，可以通过低分辨率的结果指导合成高分辨率下的插帧。



# BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation（cvpr2023）

## 解决问题
- 由于大幅度运动和小物体对光流准确性的影响，在高分辨率（如4K）视频上的插帧仍面临极大挑战。大部分网络都是在Vimeo90K上训练的，这个数据集分辨率为448 * 256。这些网络在当前比较流行的4K画质下表现不佳
- 对于由粗到细的策略：粗尺度虽然可以较好地处理大幅度运动，但是同时，粗尺度的运动误差也会传播到精细尺度。


## 解决方案
- 提出基于bilateral cross attention的**双向transform**。提出blockwise bilateral cost volumes用于精细化运动
![](biformer1.png)
- 首先下采样两个输入帧并通过BiFormer预测全局运动场$V_{t→0}^G$ 和$V_{t→1}^G$ ,然后两次上采样得到 $V_{t→0}$ 和$V_{t→1}$ ，最后使用symmetric bilateral motion model合成中间帧
![](biformer2.png)
- Global Feature Extraction: 采用Twins架构的transform encoder层
- Bilateral correlation:光流估计输入帧之间的matching cost。
- ![](biformer3.png)attention模块包括三种attention块。
	- BCA-A ：Cross attention，query为feature map $F_0$,而key和value为feature map $F_1$ 
	- ![](biformer4.png)
- ![](biformer6.png)上采样模块


# Exploring Discontinuity for Video Frame Interpolation（cvpr2023）

## 解决问题
- 现存插帧任务基于运动连续。但是视频中包含许多包含各种非自然的物体（徽标，用户界面，字幕）和非连续的运动,本文提出了三种方法来提高现有深度学习插帧框架的鲁棒性
- 感觉本文主要是针对如前后两帧中保持不变的物体或前后两帧完全不一样的物体的插值进行了优化
- ![](ed2.png)

## 解决方法
- 新的数据增强方法（FTM）：
	- Figure Mixing：固定随机数字增强
	- Text Mixing：不连续运动随机文本增强
- 一个轻量级的模块（discontinuity map):用来判断每个输出的像素点的运动是连续的还是不连续的
	- ![](ed1.png) 
- 一个新的损失函数：用于评估discontinuity map



# Frame Interpolation Transformer and Uncertainty Guidance(cvpr2023)
## 解决问题


## 解决方法
- 基于transform架构