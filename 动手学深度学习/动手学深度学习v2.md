GRU：门控循环单元
- 更新门：能关注的机制
- 重置门：把不重要的遗忘
- 门：$$R_t = \sigma(X_tW_{xr} +H_{t-1}W_{hr} + b_r)$$ $$Z_t = \sigma(X_tW_{xz} + H_{t-1}W_{hz} + b_z$$ 其中$R_t$ 为重置门，$Z_t$ 为更新门。$$\tilde{H_t} = tanh(X_tW_{xh} + (R_t \bigodot H_{t-1})W_{hh} + b_h)$$ 其中$\tilde{h_t}$ 为候选隐藏状态。 
- 那么真正的隐状态呢: $$H_t = Z_t \bigodot H_{t-1} + (1 - Z_t) \bigodot \tilde{H_t}$$ ![[Pasted image 20230823163742.png]]


LSTM
- 忘记门：将值朝0减少
- 输入门：决定是不是忽略掉输入数据
- 输出门：决定是不是使用隐状态
$$I_t = \sigma(X_tW_{xi} + H_{t-1}W_{hi} + b_i)$$ $$F_t = \sigma (X_tW_{xf} + H_{t-1}W_{hf} + b_f)$$ $$O_t = \sigma(X_tW_{xo} + H_{t-1}W_{ho} + b_o)$$ 候选记忆单元：$$\tilde{C_t} = tanh(X_tW_{xc} + H_{t-1}W_{hc} + b_c)$$ 记忆单元：$$C_t = F_t \bigodot C_{t-1} + I_t \bigodot \tilde{C_t}$$ 隐状态：$$H_t = O_t \bigodot tanh(C_t)$$ 

编码器解码器：
![[Pasted image 20230823203649.png]]
- 编码器处理输入
- 解码器生成输出 （解码器也可以用于输入）


seq2seq：
![[Pasted image 20230823204920.png]]
- 编码器是一个RNN，读取输入的句子
- 解码器使用另外一个RNN输出


注意力机制：
- 不随意线索：物体中比较突出的部分，比如一群男人中的一个女人
- 随意线索：有意识地想要关注的东西
- 注意力机制主要考虑随意线索
	- 随意线索被称为查询(query)
	- 每个输入是一个值(value)和不随意线索(key)的对
	- 通过注意力池化层来有偏向性地选择某些输入![[Pasted image 20230824103446.png]]
- 非参注意力池化层(不需要学任何东西)：
	- 给定数据$(x_i, y_i), i = 1,...,n$ ,其中x为key，y为val。
	- 平均池化：$f(x) = \frac{1}{n}\sum_i y_i$ 这里的x为query
	- 更好的方案：$$f(x) = \sum_{i=1}^{n} \frac{K(x-x_i)}{\sum_{j=1}^{n}K(x-x_j)} y_i$$ 其中x为query，$x_j$ 为key， $y_i$ 为value
		- 对于K：如果使用高斯核$K(u) = \frac{1}{\sqrt{2\pi}}exp(-\frac{u^2} {2})$   那么对应的$f(x) = \sum_{i=1}^{n} softmax\big(-\frac{1}{2}(x-x_i)^2\big) y_i$
- 参数化的注意力机制：
	- 在之前的基础上引入可以学习的W:$f(x) = \sum_{i=1}^{n} softmax\Big(-\frac{1}{2}\big((x-x_i)w \big)^2\Big) y_i$ 
	- 可以一般底写作$f(x) = \sum_{i}\alpha (x, x_i)y_i$ 这里的$\alpha (x, x_i)$ 为注意力权重



注意力分数：
- $f(x) = \sum_{i}\alpha (x, x_i)y_i = \sum_{i=1}^{n} softmax\big(-\frac{1}{2}(x-x_i)^2\big) y_i$ 其中$\alpha$为注意力权重，$-\frac{1}{2}(x-x_i)^2$ 为注意力分数![[Pasted image 20230828100954.png]]
- 将k,v拓展到高维度：
	- 注意力池化层：$$f\big(q, (k_1,v_1),...,(k_m, v_m)\big) = \sum_{i=1}^{m}\alpha(q,k_i)v_i$$ $$\alpha(q,k_i) = softmax\big(a(q,k_i)\big) = \frac{exp\big(a(q,k_i))}{\sum_{j=1}^{m}exp\big(a(q,k_j)\big)}$$
	- Additive Attention:
		- 可学参数：$W_k \in R^{h*k}$  , $W_q \in R^{h*k}$ , $v \in R^h$ : $a(k,v) = v^Ttanh(W_kk + w_qq)$  
	- Scaled Dot-Product Attention:
		- 如果query和key都是同样长度的$q,k_i \in R^d$ ,$a(q,k_i) = <q, k_i>/\sqrt d$   


自注意力：
- 自注意力池化层将$x_i$ 当做key，value，query来对序列抽取特征得到$y_1, y_2,...y_n$ ,其中$y_i = f\big(x_i(x_1,x_1),...(x_n, x_n)\big)$ 

Transformer
- ![[Pasted image 20230829191116.png]]
- 多头注意力：对于同一key，value，query，希望抽取不同的信息。$$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$ $d_k$ 是query和Key的长度，认为二者相等。 Q：是query矩阵，可以认为是多个query concat到一起，其shape为$(n, d_k)$ , K是所有key的concat，其shape为$(m, d_k)$ ，所以$QK^T$ 是一个n*m的矩阵，每一行是一个query。而V的大小为$m * d_v$ ,计算结果变成$n * d_v$ 
	- 假设我的输入句子长度为n，那么输入为n的长为d的向量（batchsize为1）
	- q,k,v的size为$(BatchSize, SeqLen, Nhead * HeadDim)$ 
- 基于位置的前馈网络：将输入形状由（b,n,d）变换成（bn,d），作用两个全连接，输出由(bn,d)变成(b,n,d)。
	- 是一个MLP， $FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$ 
- 层归一化：![[Pasted image 20230829192537.png]]
	- bachnorm:把每一个特征变成均值为0，方差为1
	- layernorm：把每一个样本变成均值为0，方差为1
	- layernorm计算过程：
		- 先在单个样本计算每一层的均值$\mu^l = \frac{1}{H}\sum^H_{i=1}a^l_i$ ,和标准差$\sigma^l = \sqrt{\frac{1}{H}\sum^H_{i=1}(a^l_i - \mu^l)^2}$ 其中H为当前layer的大小hidden units数量
		- 然后对每个值应用变换$\bar{a}^l_i = \frac{a^l_i - \mu^l}{\sigma^l}$ 
- 信息传递：编码器中的输出$y_1,...y_n$ 作为解码器中第i个Transform块中多头注意力的key和value

BERT
- 只有编码器的Transform(多层双向Transform编码器)
- 两个步骤：pre-training, fine-tuning
- 修改的参数：
	- L : Transformer blocks
	- H : hidden size
	- A : the number of self-attention heads