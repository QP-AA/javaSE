首先，我们选择的应用场景是相机和物体的共同运动。在该场景下并没有与之对应的包含事件的数据集。但是在传统去模糊数据集中包含这种场景。
我们在该数据集（BSD）上进行了相关测试，得到的==PSNR值为32.5485，SSIS值为0.9351==。由于这些指标并不能直观的反应去模糊的质量。所以我们生成对应视频并进行了比较。[ESTRNN-022](file:///D:\eventCamera\results\2023_08_21_16_24_44_ESTRNN_BSD\BSD_ESTRNN_test\022.avi) [ESTRNN-119](file:///D:\eventCamera\results\2023_08_21_16_24_44_ESTRNN_BSD\BSD_ESTRNN_test\119.avi) 
通过上面观察可以发现：在传统视频去模糊领域，包含相对运动的场景也已经得到了比较好的处理。
紧接着我们在我们的数据集上进行了相关测试。我们调整了==光照、曝光时间、平均帧数、场景内物体==等变量。得到的结果如下：==PSNR值为30.5999， SSIS值为0.944==，这个结果看上去和BSD小有差距但是不大。但是恢复出来的视频质量却非常不好。[mytest-002](file:///D:\eventCamera\results\bad001\mytest_ESTRNN_test\002.avi) ,[mytest-003](file:///D:\eventCamera\results\2023_08_22_15_42_34_ESTRNN_mytest\mytest_ESTRNN_test\003.avi) [mytest014](file:///"D:\eventCamera\results\motion003\mytest_ESTRNN_test\014.avi")。这在一定程度上说明了该方法的泛化性不是很好。同时我们也怀疑是否是我们的数据集采集的不够好才导致了这一问题的发生。因此我们测试了其在DVD数据集上包含共同运动的场景的结果。结果显示：==PSNR为29.9368，SSIS为0.8131== 。[DVD-005](file:///D:\eventCamera\results\dvd001\mytest_ESTRNN_test\004.avi) ==PSNR:35.5050, SSIS:0.9719== 。[DVD006](file:///D:\eventCamera\results\dvd002\mytest_ESTRNN_test\006.avi) 。从上面的两个视频可以看出，共同运动场景确实有着一定的提升空间。
接下来我换了另外一种思路来验证该场景的可行性：采集其他场景的视频做去模糊处理。如果结果很好，这说明相对运动这个场景确实可以做，如果结果同样不好，这只能说明模型的泛化性较差导致了去模糊的结果不够好，而不是在共同运动场景下结果不好。由此，我们得到只有物体运动的情况下，对应==PSNR为32.0479， SSIS为0.9401==，PSNR值与共同运动相比略有提高，但是恢复出来的视频效果也同样差强人意[mytest009](file:///"D:\eventCamera\results\none-motion002\mytest_ESTRNN_test\010.avi") 。以及另外的一组视频，对应==PSNR为29.85，SSIM为0.9299== [mytest011](file:///"D:\eventCamera\results\none-motion003\mytest_ESTRNN_test\012.avi") 。 由此，我们得出只要物体存在一定幅度的运动，该模型的视频去模糊效果都有一定的缺陷。
接着我们采用同一模型检验事件的加入是否会对去模糊的结果有所提升

| name    | PSNR(EVENT) | SSIM(EVENT) | PSNR       | SSIS   |     |
| ------- | ----------- | ----------- | ---------- | ------ | --- |
| video1  | ==28.785==  | 0.9222      | 28.723     | 0.9225 |     |
| video2  | 26.999      | 0.8819      | ==27.191== | 0.881  |     |
| video3  | 32.533      | 0.9525      | ==32.899== | 0.9537 |     |
| video4  | 34.83       | 0.9629      | ==35.306== | 0.9644 |     |
| video5  | 37.759      | 0.9641      | ==38.653== | 0.9665 |     |
| video6  | 36.777      | 0.9597      | ==37.852== | 0.9632 |     |
| video7  | 31.735      | 0.9432      | ==32.326== | 0.9447 |     |
| video8  | 33.33       | 0.9502      | ==33.86==  | 0.951  |     |
| video9  | 33.743      | 0.9439      | ==35.085== | 0.9494 |     |
| video10 | 32.765      | 0.9383      | ==34.105== | 0.9447 |     |
| video11 | 29.716      | 0.9051      | ==31.624== | 0.92   |     |
| video12 | 30.913      | 0.921       | ==32.862== | 0.9338 |     |
| AVG     | 33.081      | 0.9439      | ==34.063==     | 0.9454       |     |

从以上表格可以看出，再加入了event之后，效果反而没有之前好了。再从恢复出来的视频看[video2](file:///"D:\eventCamera\results\D2Net\video2.avi") [video4](file:///"D:\eventCamera\results\D2Net\video4.avi") [video7](file:///"D:\eventCamera\results\D2Net\video7.avi")   [video12](file:///"D:\eventCamera\results\D2Net\video12.avi") .有无事件的效果差距微乎其微。
综上，得出结论：
- 共同运动确实是一个目前还没有解决的巨大难题，但是它只是作为运动导致模糊的一个分支，与只有相机运动和只有物体运动相比并没有什么特别特别的地方
- 真实事件对去模糊的效果帮助并不是很大
- 由于硬件问题，采集共同运动数据集的难度较大